{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inception-v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+LqgqZxdhWzSsMPBdbADs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dream-Nine/MachineLearning/blob/master/Inception_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCg8qvgI3YfD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn # All neural network modules, nn.Linear, nn.ReLU, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha5X4Yn5_O5L"
      },
      "source": [
        "NUM_CLASSES = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czGRevR7b9zB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56fa8bd-e143-438b-de54-ca172d49dee4"
      },
      "source": [
        "class conv_block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, **kwargs):\n",
        "    super(conv_block, self).__init__()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
        "    self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.relu(self.batchnorm(self.conv(x)))\n",
        "\n",
        "class Inception_block(nn.Module):\n",
        "  def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool):\n",
        "    super(Inception_block, self).__init__()\n",
        "\n",
        "    self.branch1 = conv_block(in_channels, out_1x1, kernel_size=1)\n",
        "    self.branch2 = nn.Sequential(\n",
        "        conv_block(in_channels, red_3x3, kernel_size=1),\n",
        "        conv_block(red_3x3, out_3x3, kernel_size=3, stride=1, padding=1)\n",
        "    )\n",
        "    self.branch3 = nn.Sequential(\n",
        "        conv_block(in_channels, red_5x5, kernel_size=1),\n",
        "        conv_block(red_5x5, out_5x5, kernel_size=5, stride=1, padding=2)\n",
        "    )\n",
        "    self.branch4 = nn.Sequential(\n",
        "        nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "        conv_block(in_channels, out_1x1pool, kernel_size=1, stride=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    #               N(batch size) x #filters x H x W  \n",
        "    # Respectively, dim is 0th,     1st,      2nd, 3rd =>  dim=1은 filter-wise로 concatenate하라는 것을 의미한다.\n",
        "    return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], dim=1)\n",
        "     \n",
        "class GoogLeNet(nn.Module):\n",
        "  def __init__(self, in_channels=3, num_classes=NUM_CLASSES):\n",
        "    super(GoogLeNet, self).__init__()\n",
        "\n",
        "    self.conv1 = conv_block(in_channels=in_channels, out_channels=64, kernel_size=(7,7), stride=(2,2), padding=(3,3))\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    self.conv2 = conv_block(64, 192, kernel_size=3, stride=1, padding=1)\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    # In this ordelr : In_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool\n",
        "    self.inception3a = Inception_block(192, 64, 96,128, 16, 32, 32)\n",
        "    self.inception3b = Inception_block(256, 128, 128, 192, 32, 96, 64)\n",
        "    self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    self.inception4a = Inception_block(480, 192, 96, 208, 16, 48, 64)\n",
        "    self.inception4b = Inception_block(512, 160, 112, 224, 24, 64, 64)\n",
        "    self.inception4c = Inception_block(512, 128, 128, 256, 24, 64, 64)\n",
        "    self.inception4d = Inception_block(512, 112, 144, 288, 32, 64, 64)\n",
        "    self.inception4e = Inception_block(528, 256, 160, 320, 32, 128, 128)\n",
        "    self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    self.inception5a = Inception_block(832, 256, 160, 320, 32, 128, 128)\n",
        "    self.inception5b = Inception_block(832, 384, 192, 384, 48, 128, 128)\n",
        "    # If we use input images which size is 32 * 32, then we need not use avg pool\n",
        "    self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
        "    self.dropout = nn.Dropout(p=0.4)\n",
        "    self.fc1 = nn.Linear(1024, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.maxpool1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.maxpool2(x)\n",
        "\n",
        "    x = self.inception3a(x)\n",
        "    x = self.inception3b(x)\n",
        "    x = self.maxpool3(x)\n",
        "\n",
        "    x = self.inception4a(x)\n",
        "    x = self.inception4b(x)\n",
        "    x = self.inception4c(x)\n",
        "    x = self.inception4d(x)\n",
        "    x = self.inception4e(x)\n",
        "    x = self.maxpool4(x)\n",
        "\n",
        "    x = self.inception5a(x)\n",
        "    x = self.inception5b(x)\n",
        "    x = x.reshape(x.shape[0], -1) # N x 1 x 1을 N x 1로 reshape\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc1(x)\n",
        "\n",
        "    return x\n",
        "# How to check the size of output tensor(i.e., whether the network works wll or not)\n",
        "if __name__ == '__main__':\n",
        "  x = torch.randn(3, 3, 32, 32)\n",
        "  model = GoogLeNet()\n",
        "  print(model(x).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrkkNaKdKDkU",
        "outputId": "a9c1baba-ece2-485b-8467-03aef0417feb"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20RmWf8bKGEC"
      },
      "source": [
        "history = dict()\n",
        "history['train_loss'] = list()\n",
        "history['test_loss'] = list()\n",
        "history['test_acc'] = list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV9B6UosKOcK"
      },
      "source": [
        "device = 'cuda'\n",
        "\n",
        "net = GoogLeNet()\n",
        "net = net.to(device)\n",
        "net = torch.nn.DataParallel(net)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\n[ Train epoch: %d ]' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        benign_outputs = net(inputs)\n",
        "        loss = criterion(benign_outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = benign_outputs.max(1)\n",
        "\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('\\nCurrent batch:', str(batch_idx))\n",
        "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
        "            print('Current benign train loss:', loss.item())\n",
        "    \n",
        "    total_train_acc =  round(correct / total, 3)\n",
        "\n",
        "    print('\\nTotal benign train accuarcy:', total_train_acc)\n",
        "    print('Total benign train loss:', train_loss)\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    print('\\n[ Test epoch: %d ]' % epoch)\n",
        "    net.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    test_loss = loss / total\n",
        "    test_acc = 100. * correct / total\n",
        "    print('\\nTest accuarcy:', test_acc)\n",
        "    print('Test average loss:', test_loss)\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lr = learning_rate\n",
        "    if epoch >= 100:\n",
        "        lr /= 10\n",
        "    if epoch >= 150:\n",
        "        lr /= 10\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y61Wu3TKRmv",
        "outputId": "46fdc8da-8dde-4d73-caec-b506a333c6eb"
      },
      "source": [
        "for epoch in range(1, 201):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    history['train_loss'].append(train(epoch))\n",
        "    test_loss, test_acc = test(epoch)\n",
        "    history['test_loss'].append(test_loss)\n",
        "    history['test_acc'].append(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[ Train epoch: 1 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "\n",
            "Total benign train accuarcy: 0.558\n",
            "Total benign train loss: 486.8296448588371\n",
            "\n",
            "[ Test epoch: 8 ]\n",
            "\n",
            "Test accuarcy: 56.31\n",
            "Test average loss: 0.012506600123643874\n",
            "\n",
            "[ Train epoch: 9 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0680410861968994\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6171875\n",
            "Current benign train loss: 1.1802432537078857\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5234375\n",
            "Current benign train loss: 1.3302010297775269\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.5390625\n",
            "Current benign train loss: 1.2021068334579468\n",
            "\n",
            "Total benign train accuarcy: 0.591\n",
            "Total benign train loss: 456.73240119218826\n",
            "\n",
            "[ Test epoch: 9 ]\n",
            "\n",
            "Test accuarcy: 51.7\n",
            "Test average loss: 0.014727021133899688\n",
            "\n",
            "[ Train epoch: 10 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9912496209144592\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6484375\n",
            "Current benign train loss: 1.0112876892089844\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.640625\n",
            "Current benign train loss: 1.1115227937698364\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9045267105102539\n",
            "\n",
            "Total benign train accuarcy: 0.609\n",
            "Total benign train loss: 438.6641327738762\n",
            "\n",
            "[ Test epoch: 10 ]\n",
            "\n",
            "Test accuarcy: 53.57\n",
            "Test average loss: 0.01327304584980011\n",
            "\n",
            "[ Train epoch: 11 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6015625\n",
            "Current benign train loss: 1.0874708890914917\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.734375\n",
            "Current benign train loss: 0.9276720881462097\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6484375\n",
            "Current benign train loss: 1.0083558559417725\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7265625\n",
            "Current benign train loss: 0.9157750010490417\n",
            "\n",
            "Total benign train accuarcy: 0.63\n",
            "Total benign train loss: 414.2326909303665\n",
            "\n",
            "[ Test epoch: 11 ]\n",
            "\n",
            "Test accuarcy: 57.79\n",
            "Test average loss: 0.012291355258226395\n",
            "\n",
            "[ Train epoch: 12 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6640625\n",
            "Current benign train loss: 0.9529977440834045\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0877838134765625\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.640625\n",
            "Current benign train loss: 0.9943689107894897\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6796875\n",
            "Current benign train loss: 0.8877732753753662\n",
            "\n",
            "Total benign train accuarcy: 0.648\n",
            "Total benign train loss: 397.78075551986694\n",
            "\n",
            "[ Test epoch: 12 ]\n",
            "\n",
            "Test accuarcy: 61.08\n",
            "Test average loss: 0.011825322711467743\n",
            "\n",
            "[ Train epoch: 13 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6484375\n",
            "Current benign train loss: 1.0757311582565308\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0431872606277466\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8304574489593506\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9626255631446838\n",
            "\n",
            "Total benign train accuarcy: 0.668\n",
            "Total benign train loss: 378.44190216064453\n",
            "\n",
            "[ Test epoch: 13 ]\n",
            "\n",
            "Test accuarcy: 60.54\n",
            "Test average loss: 0.011357768887281417\n",
            "\n",
            "[ Train epoch: 14 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6484375\n",
            "Current benign train loss: 1.033866286277771\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.640625\n",
            "Current benign train loss: 0.989098846912384\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0118756294250488\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6328125\n",
            "Current benign train loss: 1.0396617650985718\n",
            "\n",
            "Total benign train accuarcy: 0.68\n",
            "Total benign train loss: 365.98380839824677\n",
            "\n",
            "[ Test epoch: 14 ]\n",
            "\n",
            "Test accuarcy: 61.87\n",
            "Test average loss: 0.011181189519166947\n",
            "\n",
            "[ Train epoch: 15 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6171875\n",
            "Current benign train loss: 1.035561442375183\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7578125\n",
            "Current benign train loss: 0.777023196220398\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8748951554298401\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.671875\n",
            "Current benign train loss: 0.9935332536697388\n",
            "\n",
            "Total benign train accuarcy: 0.693\n",
            "Total benign train loss: 351.73206347227097\n",
            "\n",
            "[ Test epoch: 15 ]\n",
            "\n",
            "Test accuarcy: 60.09\n",
            "Test average loss: 0.011901387006044388\n",
            "\n",
            "[ Train epoch: 16 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6796875\n",
            "Current benign train loss: 0.7951424717903137\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0586315393447876\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6953125\n",
            "Current benign train loss: 0.9180258512496948\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9371340274810791\n",
            "\n",
            "Total benign train accuarcy: 0.701\n",
            "Total benign train loss: 340.784831404686\n",
            "\n",
            "[ Test epoch: 16 ]\n",
            "\n",
            "Test accuarcy: 63.96\n",
            "Test average loss: 0.011320790529251099\n",
            "\n",
            "[ Train epoch: 17 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.7109375\n",
            "Current benign train loss: 0.7811052203178406\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7511632442474365\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7109375\n",
            "Current benign train loss: 0.8447577953338623\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6953125\n",
            "Current benign train loss: 0.9903287887573242\n",
            "\n",
            "Total benign train accuarcy: 0.713\n",
            "Total benign train loss: 329.2421469092369\n",
            "\n",
            "[ Test epoch: 17 ]\n",
            "\n",
            "Test accuarcy: 60.15\n",
            "Test average loss: 0.01160827271938324\n",
            "\n",
            "[ Train epoch: 18 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6484375\n",
            "Current benign train loss: 1.0337682962417603\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6640625\n",
            "Current benign train loss: 0.9998689293861389\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.734375\n",
            "Current benign train loss: 0.7024044394493103\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6953125\n",
            "Current benign train loss: 0.8231496810913086\n",
            "\n",
            "Total benign train accuarcy: 0.723\n",
            "Total benign train loss: 319.2307633757591\n",
            "\n",
            "[ Test epoch: 18 ]\n",
            "\n",
            "Test accuarcy: 63.44\n",
            "Test average loss: 0.011034368634223938\n",
            "\n",
            "[ Train epoch: 19 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.7578125\n",
            "Current benign train loss: 0.7539123296737671\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7713663578033447\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9905978441238403\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.703125\n",
            "Current benign train loss: 0.8613506555557251\n",
            "\n",
            "Total benign train accuarcy: 0.731\n",
            "Total benign train loss: 309.6203860640526\n",
            "\n",
            "[ Test epoch: 19 ]\n",
            "\n",
            "Test accuarcy: 68.97\n",
            "Test average loss: 0.009383545565605163\n",
            "\n",
            "[ Train epoch: 20 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8801401257514954\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.734375\n",
            "Current benign train loss: 0.8520208597183228\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.703125\n",
            "Current benign train loss: 0.8548253774642944\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7109375\n",
            "Current benign train loss: 0.8153715133666992\n",
            "\n",
            "Total benign train accuarcy: 0.736\n",
            "Total benign train loss: 304.22963693737984\n",
            "\n",
            "[ Test epoch: 20 ]\n",
            "\n",
            "Test accuarcy: 62.81\n",
            "Test average loss: 0.011422470104694367\n",
            "\n",
            "[ Train epoch: 21 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.734375\n",
            "Current benign train loss: 0.7784382700920105\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7591379880905151\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7421875\n",
            "Current benign train loss: 0.806194543838501\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.802260160446167\n",
            "\n",
            "Total benign train accuarcy: 0.742\n",
            "Total benign train loss: 298.2911424636841\n",
            "\n",
            "[ Test epoch: 21 ]\n",
            "\n",
            "Test accuarcy: 63.44\n",
            "Test average loss: 0.011579593557119369\n",
            "\n",
            "[ Train epoch: 22 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.639434278011322\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7578125\n",
            "Current benign train loss: 0.7258205413818359\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6908411383628845\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7265625\n",
            "Current benign train loss: 0.7017515301704407\n",
            "\n",
            "Total benign train accuarcy: 0.746\n",
            "Total benign train loss: 290.99409729242325\n",
            "\n",
            "[ Test epoch: 22 ]\n",
            "\n",
            "Test accuarcy: 68.01\n",
            "Test average loss: 0.009459113389253617\n",
            "\n",
            "[ Train epoch: 23 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.6084010601043701\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7421875\n",
            "Current benign train loss: 0.7264308929443359\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7109375\n",
            "Current benign train loss: 0.7462572455406189\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.7413744926452637\n",
            "\n",
            "Total benign train accuarcy: 0.752\n",
            "Total benign train loss: 286.8261751830578\n",
            "\n",
            "[ Test epoch: 23 ]\n",
            "\n",
            "Test accuarcy: 70.31\n",
            "Test average loss: 0.0087124351978302\n",
            "\n",
            "[ Train epoch: 24 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.7578125\n",
            "Current benign train loss: 0.7056537866592407\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.765625\n",
            "Current benign train loss: 0.7948945164680481\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7296847701072693\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7203299403190613\n",
            "\n",
            "Total benign train accuarcy: 0.752\n",
            "Total benign train loss: 283.3332216143608\n",
            "\n",
            "[ Test epoch: 24 ]\n",
            "\n",
            "Test accuarcy: 69.68\n",
            "Test average loss: 0.009074906075000763\n",
            "\n",
            "[ Train epoch: 25 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.579036295413971\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8092130422592163\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.6038099527359009\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.765625\n",
            "Current benign train loss: 0.7342448234558105\n",
            "\n",
            "Total benign train accuarcy: 0.76\n",
            "Total benign train loss: 278.68512031435966\n",
            "\n",
            "[ Test epoch: 25 ]\n",
            "\n",
            "Test accuarcy: 61.94\n",
            "Test average loss: 0.012074591892957688\n",
            "\n",
            "[ Train epoch: 26 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.6318237781524658\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.703125\n",
            "Current benign train loss: 0.8809823393821716\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7421875\n",
            "Current benign train loss: 0.7996983528137207\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7734375\n",
            "Current benign train loss: 0.7060543298721313\n",
            "\n",
            "Total benign train accuarcy: 0.762\n",
            "Total benign train loss: 272.96409249305725\n",
            "\n",
            "[ Test epoch: 26 ]\n",
            "\n",
            "Test accuarcy: 67.42\n",
            "Test average loss: 0.009933931660652161\n",
            "\n",
            "[ Train epoch: 27 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.795505166053772\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7282149195671082\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7109375\n",
            "Current benign train loss: 0.7003105878829956\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7265625\n",
            "Current benign train loss: 0.7352776527404785\n",
            "\n",
            "Total benign train accuarcy: 0.765\n",
            "Total benign train loss: 270.37373119592667\n",
            "\n",
            "[ Test epoch: 27 ]\n",
            "\n",
            "Test accuarcy: 69.15\n",
            "Test average loss: 0.009127046269178391\n",
            "\n",
            "[ Train epoch: 28 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.6439439058303833\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7734375\n",
            "Current benign train loss: 0.6084200143814087\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7421875\n",
            "Current benign train loss: 0.76751309633255\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.652746856212616\n",
            "\n",
            "Total benign train accuarcy: 0.771\n",
            "Total benign train loss: 263.5527740120888\n",
            "\n",
            "[ Test epoch: 28 ]\n",
            "\n",
            "Test accuarcy: 73.25\n",
            "Test average loss: 0.007744781458377838\n",
            "\n",
            "[ Train epoch: 29 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.609870195388794\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7265625\n",
            "Current benign train loss: 0.7751621007919312\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7734375\n",
            "Current benign train loss: 0.6050753593444824\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6484375\n",
            "Current benign train loss: 0.951795220375061\n",
            "\n",
            "Total benign train accuarcy: 0.774\n",
            "Total benign train loss: 262.1548921763897\n",
            "\n",
            "[ Test epoch: 29 ]\n",
            "\n",
            "Test accuarcy: 60.34\n",
            "Test average loss: 0.012785991489887237\n",
            "\n",
            "[ Train epoch: 30 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.7578125\n",
            "Current benign train loss: 0.657477855682373\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5493411421775818\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8640248775482178\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7265625\n",
            "Current benign train loss: 0.7247879505157471\n",
            "\n",
            "Total benign train accuarcy: 0.778\n",
            "Total benign train loss: 256.3172830045223\n",
            "\n",
            "[ Test epoch: 30 ]\n",
            "\n",
            "Test accuarcy: 68.12\n",
            "Test average loss: 0.0097485016644001\n",
            "\n",
            "[ Train epoch: 31 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.765625\n",
            "Current benign train loss: 0.7408822178840637\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5117112994194031\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.6153567433357239\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.5650374889373779\n",
            "\n",
            "Total benign train accuarcy: 0.782\n",
            "Total benign train loss: 254.03306844830513\n",
            "\n",
            "[ Test epoch: 31 ]\n",
            "\n",
            "Test accuarcy: 71.04\n",
            "Test average loss: 0.008453722250461579\n",
            "\n",
            "[ Train epoch: 32 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6568936109542847\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7095046639442444\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5687950253486633\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5483815670013428\n",
            "\n",
            "Total benign train accuarcy: 0.782\n",
            "Total benign train loss: 252.19188690185547\n",
            "\n",
            "[ Test epoch: 32 ]\n",
            "\n",
            "Test accuarcy: 73.0\n",
            "Test average loss: 0.008158342504501342\n",
            "\n",
            "[ Train epoch: 33 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.5558570027351379\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6262324452400208\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6962816119194031\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.633663535118103\n",
            "\n",
            "Total benign train accuarcy: 0.783\n",
            "Total benign train loss: 250.2764108479023\n",
            "\n",
            "[ Test epoch: 33 ]\n",
            "\n",
            "Test accuarcy: 72.59\n",
            "Test average loss: 0.008483683562278748\n",
            "\n",
            "[ Train epoch: 34 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.6237058043479919\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.5811167359352112\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.4559529423713684\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8500510454177856\n",
            "\n",
            "Total benign train accuarcy: 0.785\n",
            "Total benign train loss: 247.6568661928177\n",
            "\n",
            "[ Test epoch: 34 ]\n",
            "\n",
            "Test accuarcy: 72.45\n",
            "Test average loss: 0.00835099783539772\n",
            "\n",
            "[ Train epoch: 35 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.572873592376709\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7421875\n",
            "Current benign train loss: 0.6854113340377808\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.58772212266922\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5587015748023987\n",
            "\n",
            "Total benign train accuarcy: 0.789\n",
            "Total benign train loss: 244.62626215815544\n",
            "\n",
            "[ Test epoch: 35 ]\n",
            "\n",
            "Test accuarcy: 70.8\n",
            "Test average loss: 0.009290817338228227\n",
            "\n",
            "[ Train epoch: 36 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.6120193600654602\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.734375\n",
            "Current benign train loss: 0.8280264139175415\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.44069617986679077\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7578125\n",
            "Current benign train loss: 0.6235024333000183\n",
            "\n",
            "Total benign train accuarcy: 0.792\n",
            "Total benign train loss: 241.64302995800972\n",
            "\n",
            "[ Test epoch: 36 ]\n",
            "\n",
            "Test accuarcy: 65.88\n",
            "Test average loss: 0.010758390325307846\n",
            "\n",
            "[ Train epoch: 37 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.5404150485992432\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5784620046615601\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7578125\n",
            "Current benign train loss: 0.6216703653335571\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.49247226119041443\n",
            "\n",
            "Total benign train accuarcy: 0.794\n",
            "Total benign train loss: 239.33541110157967\n",
            "\n",
            "[ Test epoch: 37 ]\n",
            "\n",
            "Test accuarcy: 69.81\n",
            "Test average loss: 0.009152076673507691\n",
            "\n",
            "[ Train epoch: 38 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5595462322235107\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5659723281860352\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.765625\n",
            "Current benign train loss: 0.6236893534660339\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.4638421833515167\n",
            "\n",
            "Total benign train accuarcy: 0.791\n",
            "Total benign train loss: 239.68633139133453\n",
            "\n",
            "[ Test epoch: 38 ]\n",
            "\n",
            "Test accuarcy: 73.51\n",
            "Test average loss: 0.007908723098039626\n",
            "\n",
            "[ Train epoch: 39 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5674569606781006\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7265625\n",
            "Current benign train loss: 0.8383012413978577\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5372116565704346\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.6858739256858826\n",
            "\n",
            "Total benign train accuarcy: 0.798\n",
            "Total benign train loss: 234.06419271230698\n",
            "\n",
            "[ Test epoch: 39 ]\n",
            "\n",
            "Test accuarcy: 52.68\n",
            "Test average loss: 0.019082592225074767\n",
            "\n",
            "[ Train epoch: 40 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.4950030744075775\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.48180118203163147\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.4612686336040497\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.452981561422348\n",
            "\n",
            "Total benign train accuarcy: 0.799\n",
            "Total benign train loss: 236.0330966114998\n",
            "\n",
            "[ Test epoch: 40 ]\n",
            "\n",
            "Test accuarcy: 72.82\n",
            "Test average loss: 0.007970206880569458\n",
            "\n",
            "[ Train epoch: 41 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.44877326488494873\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7578125\n",
            "Current benign train loss: 0.5739938616752625\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6203420758247375\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.734375\n",
            "Current benign train loss: 0.6765598654747009\n",
            "\n",
            "Total benign train accuarcy: 0.797\n",
            "Total benign train loss: 234.5394588112831\n",
            "\n",
            "[ Test epoch: 41 ]\n",
            "\n",
            "Test accuarcy: 68.82\n",
            "Test average loss: 0.009722619915008545\n",
            "\n",
            "[ Train epoch: 42 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.7734375\n",
            "Current benign train loss: 0.6530218720436096\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.4537072777748108\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.765625\n",
            "Current benign train loss: 0.6170173287391663\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.42308756709098816\n",
            "\n",
            "Total benign train accuarcy: 0.799\n",
            "Total benign train loss: 232.18131178617477\n",
            "\n",
            "[ Test epoch: 42 ]\n",
            "\n",
            "Test accuarcy: 67.03\n",
            "Test average loss: 0.010508742916584015\n",
            "\n",
            "[ Train epoch: 43 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.504365086555481\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.48143497109413147\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.6436073780059814\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.48562464118003845\n",
            "\n",
            "Total benign train accuarcy: 0.802\n",
            "Total benign train loss: 230.19130438566208\n",
            "\n",
            "[ Test epoch: 43 ]\n",
            "\n",
            "Test accuarcy: 66.54\n",
            "Test average loss: 0.011187921720743179\n",
            "\n",
            "[ Train epoch: 44 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.580577552318573\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5383558869361877\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.407638818025589\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5764131546020508\n",
            "\n",
            "Total benign train accuarcy: 0.803\n",
            "Total benign train loss: 227.93318811058998\n",
            "\n",
            "[ Test epoch: 44 ]\n",
            "\n",
            "Test accuarcy: 70.93\n",
            "Test average loss: 0.009101561099290847\n",
            "\n",
            "[ Train epoch: 45 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.37721022963523865\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.6293936967849731\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.703125\n",
            "Current benign train loss: 0.7035391926765442\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.5679724216461182\n",
            "\n",
            "Total benign train accuarcy: 0.805\n",
            "Total benign train loss: 227.4342379271984\n",
            "\n",
            "[ Test epoch: 45 ]\n",
            "\n",
            "Test accuarcy: 66.6\n",
            "Test average loss: 0.01057689555287361\n",
            "\n",
            "[ Train epoch: 46 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5989943146705627\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5127092599868774\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.5716904401779175\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.7351058125495911\n",
            "\n",
            "Total benign train accuarcy: 0.809\n",
            "Total benign train loss: 222.35506853461266\n",
            "\n",
            "[ Test epoch: 46 ]\n",
            "\n",
            "Test accuarcy: 71.53\n",
            "Test average loss: 0.009206299138069153\n",
            "\n",
            "[ Train epoch: 47 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.734375\n",
            "Current benign train loss: 0.7811235785484314\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7734375\n",
            "Current benign train loss: 0.5510565042495728\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.539838433265686\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.5819040536880493\n",
            "\n",
            "Total benign train accuarcy: 0.807\n",
            "Total benign train loss: 224.89454650878906\n",
            "\n",
            "[ Test epoch: 47 ]\n",
            "\n",
            "Test accuarcy: 72.43\n",
            "Test average loss: 0.00851434359550476\n",
            "\n",
            "[ Train epoch: 48 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.4486510455608368\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.4547584652900696\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.5628751516342163\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5576285719871521\n",
            "\n",
            "Total benign train accuarcy: 0.809\n",
            "Total benign train loss: 220.55025726556778\n",
            "\n",
            "[ Test epoch: 48 ]\n",
            "\n",
            "Test accuarcy: 74.56\n",
            "Test average loss: 0.007935584622621537\n",
            "\n",
            "[ Train epoch: 49 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.4764125943183899\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.5709850788116455\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.734375\n",
            "Current benign train loss: 0.7697964310646057\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5590988993644714\n",
            "\n",
            "Total benign train accuarcy: 0.812\n",
            "Total benign train loss: 218.48448804020882\n",
            "\n",
            "[ Test epoch: 49 ]\n",
            "\n",
            "Test accuarcy: 65.56\n",
            "Test average loss: 0.010970729833841324\n",
            "\n",
            "[ Train epoch: 50 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5677406787872314\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.6047108173370361\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.6442611217498779\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.568928062915802\n",
            "\n",
            "Total benign train accuarcy: 0.812\n",
            "Total benign train loss: 219.60658651590347\n",
            "\n",
            "[ Test epoch: 50 ]\n",
            "\n",
            "Test accuarcy: 64.32\n",
            "Test average loss: 0.011605123913288117\n",
            "\n",
            "[ Train epoch: 51 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.6917118430137634\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.4371997117996216\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.6432228088378906\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.6625547409057617\n",
            "\n",
            "Total benign train accuarcy: 0.81\n",
            "Total benign train loss: 221.45050102472305\n",
            "\n",
            "[ Test epoch: 51 ]\n",
            "\n",
            "Test accuarcy: 63.73\n",
            "Test average loss: 0.011005731105804443\n",
            "\n",
            "[ Train epoch: 52 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.4935505986213684\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.48767977952957153\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.6091527342796326\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.47457900643348694\n",
            "\n",
            "Total benign train accuarcy: 0.811\n",
            "Total benign train loss: 217.41500666737556\n",
            "\n",
            "[ Test epoch: 52 ]\n",
            "\n",
            "Test accuarcy: 73.53\n",
            "Test average loss: 0.008242940771579742\n",
            "\n",
            "[ Train epoch: 53 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.5267832279205322\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5528058409690857\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.6003371477127075\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.46472835540771484\n",
            "\n",
            "Total benign train accuarcy: 0.812\n",
            "Total benign train loss: 218.72752365469933\n",
            "\n",
            "[ Test epoch: 53 ]\n",
            "\n",
            "Test accuarcy: 68.93\n",
            "Test average loss: 0.010125936889648437\n",
            "\n",
            "[ Train epoch: 54 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.90625\n",
            "Current benign train loss: 0.40916919708251953\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.42620256543159485\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.5591745972633362\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.48406732082366943\n",
            "\n",
            "Total benign train accuarcy: 0.814\n",
            "Total benign train loss: 216.12781503796577\n",
            "\n",
            "[ Test epoch: 54 ]\n",
            "\n",
            "Test accuarcy: 75.3\n",
            "Test average loss: 0.007312252876162529\n",
            "\n",
            "[ Train epoch: 55 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.7734375\n",
            "Current benign train loss: 0.6527866125106812\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.3770710527896881\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5601400136947632\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.765625\n",
            "Current benign train loss: 0.7085356116294861\n",
            "\n",
            "Total benign train accuarcy: 0.815\n",
            "Total benign train loss: 215.58384880423546\n",
            "\n",
            "[ Test epoch: 55 ]\n",
            "\n",
            "Test accuarcy: 72.79\n",
            "Test average loss: 0.00821562802195549\n",
            "\n",
            "[ Train epoch: 56 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.39995887875556946\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9140625\n",
            "Current benign train loss: 0.39855140447616577\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.42938634753227234\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7421875\n",
            "Current benign train loss: 0.63737553358078\n",
            "\n",
            "Total benign train accuarcy: 0.813\n",
            "Total benign train loss: 214.56786248087883\n",
            "\n",
            "[ Test epoch: 56 ]\n",
            "\n",
            "Test accuarcy: 70.83\n",
            "Test average loss: 0.009453958916664123\n",
            "\n",
            "[ Train epoch: 57 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.35015246272087097\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.49281567335128784\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7756099700927734\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.4859520494937897\n",
            "\n",
            "Total benign train accuarcy: 0.813\n",
            "Total benign train loss: 215.46239417791367\n",
            "\n",
            "[ Test epoch: 57 ]\n",
            "\n",
            "Test accuarcy: 73.83\n",
            "Test average loss: 0.0080594462454319\n",
            "\n",
            "[ Train epoch: 58 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.5187539458274841\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5202628374099731\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.51498943567276\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.49237823486328125\n",
            "\n",
            "Total benign train accuarcy: 0.816\n",
            "Total benign train loss: 212.48932468891144\n",
            "\n",
            "[ Test epoch: 58 ]\n",
            "\n",
            "Test accuarcy: 65.75\n",
            "Test average loss: 0.01103978955745697\n",
            "\n",
            "[ Train epoch: 59 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5330123901367188\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5350357294082642\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.5451108813285828\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.7087368369102478\n",
            "\n",
            "Total benign train accuarcy: 0.818\n",
            "Total benign train loss: 210.78513446450233\n",
            "\n",
            "[ Test epoch: 59 ]\n",
            "\n",
            "Test accuarcy: 77.92\n",
            "Test average loss: 0.006618002766370774\n",
            "\n",
            "[ Train epoch: 60 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.90625\n",
            "Current benign train loss: 0.33861684799194336\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5566919445991516\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6206416487693787\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7482739686965942\n",
            "\n",
            "Total benign train accuarcy: 0.821\n",
            "Total benign train loss: 210.91651090979576\n",
            "\n",
            "[ Test epoch: 60 ]\n",
            "\n",
            "Test accuarcy: 68.93\n",
            "Test average loss: 0.009706023466587067\n",
            "\n",
            "[ Train epoch: 61 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.5235950946807861\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5919113159179688\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8984375\n",
            "Current benign train loss: 0.3861580789089203\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5296164751052856\n",
            "\n",
            "Total benign train accuarcy: 0.819\n",
            "Total benign train loss: 210.6079375743866\n",
            "\n",
            "[ Test epoch: 61 ]\n",
            "\n",
            "Test accuarcy: 76.67\n",
            "Test average loss: 0.007072691488265991\n",
            "\n",
            "[ Train epoch: 62 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5622875690460205\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.49022847414016724\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.4687343239784241\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.46051204204559326\n",
            "\n",
            "Total benign train accuarcy: 0.822\n",
            "Total benign train loss: 207.70054104924202\n",
            "\n",
            "[ Test epoch: 62 ]\n",
            "\n",
            "Test accuarcy: 76.28\n",
            "Test average loss: 0.007137494707107544\n",
            "\n",
            "[ Train epoch: 63 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.49278515577316284\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.44121024012565613\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.40563735365867615\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7734375\n",
            "Current benign train loss: 0.6472103595733643\n",
            "\n",
            "Total benign train accuarcy: 0.819\n",
            "Total benign train loss: 210.0503273010254\n",
            "\n",
            "[ Test epoch: 63 ]\n",
            "\n",
            "Test accuarcy: 77.8\n",
            "Test average loss: 0.006604231902956963\n",
            "\n",
            "[ Train epoch: 64 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5271282196044922\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.529017448425293\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.6516990661621094\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7734375\n",
            "Current benign train loss: 0.5088878273963928\n",
            "\n",
            "Total benign train accuarcy: 0.822\n",
            "Total benign train loss: 207.3404092490673\n",
            "\n",
            "[ Test epoch: 64 ]\n",
            "\n",
            "Test accuarcy: 74.93\n",
            "Test average loss: 0.007842869049310685\n",
            "\n",
            "[ Train epoch: 65 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5469797849655151\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.5650640726089478\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5563735365867615\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.427080899477005\n",
            "\n",
            "Total benign train accuarcy: 0.819\n",
            "Total benign train loss: 208.9973857998848\n",
            "\n",
            "[ Test epoch: 65 ]\n",
            "\n",
            "Test accuarcy: 59.18\n",
            "Test average loss: 0.01375304194688797\n",
            "\n",
            "[ Train epoch: 66 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.46958622336387634\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.48225581645965576\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.45001694560050964\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.5577890276908875\n",
            "\n",
            "Total benign train accuarcy: 0.82\n",
            "Total benign train loss: 207.85355147719383\n",
            "\n",
            "[ Test epoch: 66 ]\n",
            "\n",
            "Test accuarcy: 71.27\n",
            "Test average loss: 0.00893078823685646\n",
            "\n",
            "[ Train epoch: 67 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.44799917936325073\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.4777330160140991\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7109375\n",
            "Current benign train loss: 0.7796618342399597\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.42204397916793823\n",
            "\n",
            "Total benign train accuarcy: 0.822\n",
            "Total benign train loss: 206.59803181886673\n",
            "\n",
            "[ Test epoch: 67 ]\n",
            "\n",
            "Test accuarcy: 66.38\n",
            "Test average loss: 0.010458703243732452\n",
            "\n",
            "[ Train epoch: 68 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6265901327133179\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.47292396426200867\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.5301733613014221\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6520668268203735\n",
            "\n",
            "Total benign train accuarcy: 0.82\n",
            "Total benign train loss: 206.51418885588646\n",
            "\n",
            "[ Test epoch: 68 ]\n",
            "\n",
            "Test accuarcy: 70.75\n",
            "Test average loss: 0.008504296922683716\n",
            "\n",
            "[ Train epoch: 69 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.5031644701957703\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5667151808738708\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5776253342628479\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.5769209861755371\n",
            "\n",
            "Total benign train accuarcy: 0.822\n",
            "Total benign train loss: 205.1557574570179\n",
            "\n",
            "[ Test epoch: 69 ]\n",
            "\n",
            "Test accuarcy: 75.43\n",
            "Test average loss: 0.007678419774770737\n",
            "\n",
            "[ Train epoch: 70 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.3931135833263397\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.42180007696151733\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.47803229093551636\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.4186215102672577\n",
            "\n",
            "Total benign train accuarcy: 0.824\n",
            "Total benign train loss: 204.3507126569748\n",
            "\n",
            "[ Test epoch: 70 ]\n",
            "\n",
            "Test accuarcy: 67.64\n",
            "Test average loss: 0.01038818895816803\n",
            "\n",
            "[ Train epoch: 71 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.4529065191745758\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.4054989814758301\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.4332256019115448\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.49615952372550964\n",
            "\n",
            "Total benign train accuarcy: 0.823\n",
            "Total benign train loss: 203.63354894518852\n",
            "\n",
            "[ Test epoch: 71 ]\n",
            "\n",
            "Test accuarcy: 72.15\n",
            "Test average loss: 0.008488532024621964\n",
            "\n",
            "[ Train epoch: 72 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5048768520355225\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.366607666015625\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.48731639981269836\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.5838128328323364\n",
            "\n",
            "Total benign train accuarcy: 0.822\n",
            "Total benign train loss: 204.37106481194496\n",
            "\n",
            "[ Test epoch: 72 ]\n",
            "\n",
            "Test accuarcy: 76.25\n",
            "Test average loss: 0.006945638439059257\n",
            "\n",
            "[ Train epoch: 73 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.42316991090774536\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.5633431077003479\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.4498632848262787\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5382091999053955\n",
            "\n",
            "Total benign train accuarcy: 0.827\n",
            "Total benign train loss: 201.20278626680374\n",
            "\n",
            "[ Test epoch: 73 ]\n",
            "\n",
            "Test accuarcy: 77.2\n",
            "Test average loss: 0.006882894089818001\n",
            "\n",
            "[ Train epoch: 74 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.4636543393135071\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7734375\n",
            "Current benign train loss: 0.6741325259208679\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.47377705574035645\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.485834002494812\n",
            "\n",
            "Total benign train accuarcy: 0.826\n",
            "Total benign train loss: 201.11826494336128\n",
            "\n",
            "[ Test epoch: 74 ]\n",
            "\n",
            "Test accuarcy: 74.95\n",
            "Test average loss: 0.007667872720956802\n",
            "\n",
            "[ Train epoch: 75 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5203859806060791\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5330399870872498\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.5128373503684998\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.497258722782135\n",
            "\n",
            "Total benign train accuarcy: 0.831\n",
            "Total benign train loss: 198.081745326519\n",
            "\n",
            "[ Test epoch: 75 ]\n",
            "\n",
            "Test accuarcy: 68.98\n",
            "Test average loss: 0.010024345844984055\n",
            "\n",
            "[ Train epoch: 76 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5454818606376648\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.33055052161216736\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.515338659286499\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5306430459022522\n",
            "\n",
            "Total benign train accuarcy: 0.827\n",
            "Total benign train loss: 202.12065216898918\n",
            "\n",
            "[ Test epoch: 76 ]\n",
            "\n",
            "Test accuarcy: 67.68\n",
            "Test average loss: 0.010359230941534042\n",
            "\n",
            "[ Train epoch: 77 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.4696592688560486\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.3807052969932556\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.4827735126018524\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.522230327129364\n",
            "\n",
            "Total benign train accuarcy: 0.83\n",
            "Total benign train loss: 199.66906940937042\n",
            "\n",
            "[ Test epoch: 77 ]\n",
            "\n",
            "Test accuarcy: 72.32\n",
            "Test average loss: 0.00845545808672905\n",
            "\n",
            "[ Train epoch: 78 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.4012739658355713\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.37796831130981445\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.6840252876281738\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5987606644630432\n",
            "\n",
            "Total benign train accuarcy: 0.827\n",
            "Total benign train loss: 200.14196521043777\n",
            "\n",
            "[ Test epoch: 78 ]\n",
            "\n",
            "Test accuarcy: 68.92\n",
            "Test average loss: 0.010744703435897828\n",
            "\n",
            "[ Train epoch: 79 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.47670644521713257\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.6156591176986694\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.47737544775009155\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6439053416252136\n",
            "\n",
            "Total benign train accuarcy: 0.828\n",
            "Total benign train loss: 198.82911036908627\n",
            "\n",
            "[ Test epoch: 79 ]\n",
            "\n",
            "Test accuarcy: 72.4\n",
            "Test average loss: 0.008594478106498719\n",
            "\n",
            "[ Train epoch: 80 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.42789846658706665\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.703125\n",
            "Current benign train loss: 0.6967033743858337\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.5034446120262146\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.7471395134925842\n",
            "\n",
            "Total benign train accuarcy: 0.829\n",
            "Total benign train loss: 198.26470756530762\n",
            "\n",
            "[ Test epoch: 80 ]\n",
            "\n",
            "Test accuarcy: 68.54\n",
            "Test average loss: 0.009998344278335571\n",
            "\n",
            "[ Train epoch: 81 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.4413334131240845\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.48931360244750977\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.5628608465194702\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.49211496114730835\n",
            "\n",
            "Total benign train accuarcy: 0.826\n",
            "Total benign train loss: 200.23568844795227\n",
            "\n",
            "[ Test epoch: 81 ]\n",
            "\n",
            "Test accuarcy: 65.81\n",
            "Test average loss: 0.010848320496082305\n",
            "\n",
            "[ Train epoch: 82 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.5402110815048218\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.4011492133140564\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.379740446805954\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5512951016426086\n",
            "\n",
            "Total benign train accuarcy: 0.828\n",
            "Total benign train loss: 197.65106204152107\n",
            "\n",
            "[ Test epoch: 82 ]\n",
            "\n",
            "Test accuarcy: 76.48\n",
            "Test average loss: 0.007196151101589203\n",
            "\n",
            "[ Train epoch: 83 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.38343071937561035\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.4957166612148285\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6682636141777039\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5193182826042175\n",
            "\n",
            "Total benign train accuarcy: 0.83\n",
            "Total benign train loss: 197.15470278263092\n",
            "\n",
            "[ Test epoch: 83 ]\n",
            "\n",
            "Test accuarcy: 74.95\n",
            "Test average loss: 0.00759918523132801\n",
            "\n",
            "[ Train epoch: 84 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.41458040475845337\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5070744156837463\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5027742385864258\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.48560377955436707\n",
            "\n",
            "Total benign train accuarcy: 0.829\n",
            "Total benign train loss: 200.1593588590622\n",
            "\n",
            "[ Test epoch: 84 ]\n",
            "\n",
            "Test accuarcy: 70.11\n",
            "Test average loss: 0.00935347284078598\n",
            "\n",
            "[ Train epoch: 85 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.39893198013305664\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5612271428108215\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5821999311447144\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.4589669406414032\n",
            "\n",
            "Total benign train accuarcy: 0.831\n",
            "Total benign train loss: 195.6659959256649\n",
            "\n",
            "[ Test epoch: 85 ]\n",
            "\n",
            "Test accuarcy: 77.58\n",
            "Test average loss: 0.006933183315396309\n",
            "\n",
            "[ Train epoch: 86 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.370440810918808\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.3371437191963196\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.6152750253677368\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.4033919870853424\n",
            "\n",
            "Total benign train accuarcy: 0.83\n",
            "Total benign train loss: 197.58811849355698\n",
            "\n",
            "[ Test epoch: 86 ]\n",
            "\n",
            "Test accuarcy: 70.66\n",
            "Test average loss: 0.009487226420640945\n",
            "\n",
            "[ Train epoch: 87 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.5522118806838989\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.5126095414161682\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.6235312819480896\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5839511752128601\n",
            "\n",
            "Total benign train accuarcy: 0.83\n",
            "Total benign train loss: 197.81152486801147\n",
            "\n",
            "[ Test epoch: 87 ]\n",
            "\n",
            "Test accuarcy: 74.25\n",
            "Test average loss: 0.007787115436792374\n",
            "\n",
            "[ Train epoch: 88 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.432507187128067\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.43877148628234863\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.5253089666366577\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.423446923494339\n",
            "\n",
            "Total benign train accuarcy: 0.832\n",
            "Total benign train loss: 194.07906556129456\n",
            "\n",
            "[ Test epoch: 88 ]\n",
            "\n",
            "Test accuarcy: 64.74\n",
            "Test average loss: 0.011386656999588013\n",
            "\n",
            "[ Train epoch: 89 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.4502539336681366\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.5612515211105347\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.4924123287200928\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.507931649684906\n",
            "\n",
            "Total benign train accuarcy: 0.831\n",
            "Total benign train loss: 197.74020180106163\n",
            "\n",
            "[ Test epoch: 89 ]\n",
            "\n",
            "Test accuarcy: 77.07\n",
            "Test average loss: 0.006923367002606392\n",
            "\n",
            "[ Train epoch: 90 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.54300457239151\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.44215190410614014\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.6142457127571106\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.5112395286560059\n",
            "\n",
            "Total benign train accuarcy: 0.832\n",
            "Total benign train loss: 194.5655874311924\n",
            "\n",
            "[ Test epoch: 90 ]\n",
            "\n",
            "Test accuarcy: 72.78\n",
            "Test average loss: 0.008877102464437485\n",
            "\n",
            "[ Train epoch: 91 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.34226736426353455\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5639798641204834\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.4600881040096283\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.40700626373291016\n",
            "\n",
            "Total benign train accuarcy: 0.833\n",
            "Total benign train loss: 193.37331792712212\n",
            "\n",
            "[ Test epoch: 91 ]\n",
            "\n",
            "Test accuarcy: 69.96\n",
            "Test average loss: 0.010907213431596756\n",
            "\n",
            "[ Train epoch: 92 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.4102748930454254\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.4119156301021576\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7734375\n",
            "Current benign train loss: 0.6019970774650574\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5117610096931458\n",
            "\n",
            "Total benign train accuarcy: 0.831\n",
            "Total benign train loss: 195.55983625352383\n",
            "\n",
            "[ Test epoch: 92 ]\n",
            "\n",
            "Test accuarcy: 75.05\n",
            "Test average loss: 0.007759480082988739\n",
            "\n",
            "[ Train epoch: 93 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.39296114444732666\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.6844050884246826\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.4808065593242645\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5533959865570068\n",
            "\n",
            "Total benign train accuarcy: 0.831\n",
            "Total benign train loss: 196.43193379044533\n",
            "\n",
            "[ Test epoch: 93 ]\n",
            "\n",
            "Test accuarcy: 45.58\n",
            "Test average loss: 0.022757003688812255\n",
            "\n",
            "[ Train epoch: 94 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.890625\n",
            "Current benign train loss: 0.41729477047920227\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.4862740933895111\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.4944961965084076\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8984375\n",
            "Current benign train loss: 0.27795571088790894\n",
            "\n",
            "Total benign train accuarcy: 0.833\n",
            "Total benign train loss: 193.9092634022236\n",
            "\n",
            "[ Test epoch: 94 ]\n",
            "\n",
            "Test accuarcy: 76.11\n",
            "Test average loss: 0.00736511989235878\n",
            "\n",
            "[ Train epoch: 95 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.6380499005317688\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.47384747862815857\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5675678849220276\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7734375\n",
            "Current benign train loss: 0.578113853931427\n",
            "\n",
            "Total benign train accuarcy: 0.831\n",
            "Total benign train loss: 194.13410851359367\n",
            "\n",
            "[ Test epoch: 95 ]\n",
            "\n",
            "Test accuarcy: 72.4\n",
            "Test average loss: 0.008602368259429932\n",
            "\n",
            "[ Train epoch: 96 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.90625\n",
            "Current benign train loss: 0.3196169137954712\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.647270679473877\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.5214223265647888\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.6270629167556763\n",
            "\n",
            "Total benign train accuarcy: 0.832\n",
            "Total benign train loss: 194.69559344649315\n",
            "\n",
            "[ Test epoch: 96 ]\n",
            "\n",
            "Test accuarcy: 62.76\n",
            "Test average loss: 0.012544893079996108\n",
            "\n",
            "[ Train epoch: 97 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.6107122898101807\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.4490249752998352\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7047272324562073\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.41461485624313354\n",
            "\n",
            "Total benign train accuarcy: 0.832\n",
            "Total benign train loss: 193.66987162828445\n",
            "\n",
            "[ Test epoch: 97 ]\n",
            "\n",
            "Test accuarcy: 77.61\n",
            "Test average loss: 0.006782215282320976\n",
            "\n",
            "[ Train epoch: 98 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.4502160847187042\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.47263219952583313\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7578125\n",
            "Current benign train loss: 0.7933269143104553\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.4325542747974396\n",
            "\n",
            "Total benign train accuarcy: 0.834\n",
            "Total benign train loss: 192.5814766585827\n",
            "\n",
            "[ Test epoch: 98 ]\n",
            "\n",
            "Test accuarcy: 77.15\n",
            "Test average loss: 0.0066971871167421345\n",
            "\n",
            "[ Train epoch: 99 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9296875\n",
            "Current benign train loss: 0.34329840540885925\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5400293469429016\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.42169660329818726\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.4315880239009857\n",
            "\n",
            "Total benign train accuarcy: 0.837\n",
            "Total benign train loss: 190.7803359925747\n",
            "\n",
            "[ Test epoch: 99 ]\n",
            "\n",
            "Test accuarcy: 75.34\n",
            "Test average loss: 0.007714620408415794\n",
            "\n",
            "[ Train epoch: 100 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5670036673545837\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.42808324098587036\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.890625\n",
            "Current benign train loss: 0.3878712058067322\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9140625\n",
            "Current benign train loss: 0.2735612094402313\n",
            "\n",
            "Total benign train accuarcy: 0.889\n",
            "Total benign train loss: 129.14716883003712\n",
            "\n",
            "[ Test epoch: 100 ]\n",
            "\n",
            "Test accuarcy: 85.26\n",
            "Test average loss: 0.004432116279006005\n",
            "\n",
            "[ Train epoch: 101 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.26791223883628845\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.890625\n",
            "Current benign train loss: 0.262297123670578\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.28326642513275146\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.2997996211051941\n",
            "\n",
            "Total benign train accuarcy: 0.908\n",
            "Total benign train loss: 108.24682806432247\n",
            "\n",
            "[ Test epoch: 101 ]\n",
            "\n",
            "Test accuarcy: 85.38\n",
            "Test average loss: 0.004526968768239021\n",
            "\n",
            "[ Train epoch: 102 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.3331014811992645\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.3522481620311737\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9296875\n",
            "Current benign train loss: 0.2618411183357239\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9140625\n",
            "Current benign train loss: 0.26502206921577454\n",
            "\n",
            "Total benign train accuarcy: 0.913\n",
            "Total benign train loss: 99.92830942571163\n",
            "\n",
            "[ Test epoch: 102 ]\n",
            "\n",
            "Test accuarcy: 86.17\n",
            "Test average loss: 0.004313569398224353\n",
            "\n",
            "[ Train epoch: 103 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.17208245396614075\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9140625\n",
            "Current benign train loss: 0.20832818746566772\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.2155015915632248\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9140625\n",
            "Current benign train loss: 0.3068239986896515\n",
            "\n",
            "Total benign train accuarcy: 0.917\n",
            "Total benign train loss: 95.16598249226809\n",
            "\n",
            "[ Test epoch: 103 ]\n",
            "\n",
            "Test accuarcy: 85.89\n",
            "Test average loss: 0.004348348318040371\n",
            "\n",
            "[ Train epoch: 104 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.17687061429023743\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.1388714462518692\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.2087285965681076\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8984375\n",
            "Current benign train loss: 0.2454882711172104\n",
            "\n",
            "Total benign train accuarcy: 0.922\n",
            "Total benign train loss: 89.07061303406954\n",
            "\n",
            "[ Test epoch: 104 ]\n",
            "\n",
            "Test accuarcy: 85.66\n",
            "Test average loss: 0.0044693617865443225\n",
            "\n",
            "[ Train epoch: 105 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.20080110430717468\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.21295249462127686\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9296875\n",
            "Current benign train loss: 0.20692306756973267\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.24478353559970856\n",
            "\n",
            "Total benign train accuarcy: 0.927\n",
            "Total benign train loss: 84.56059923768044\n",
            "\n",
            "[ Test epoch: 105 ]\n",
            "\n",
            "Test accuarcy: 86.27\n",
            "Test average loss: 0.004293752804398537\n",
            "\n",
            "[ Train epoch: 106 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.1436685025691986\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9296875\n",
            "Current benign train loss: 0.21020947396755219\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.2098277360200882\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9296875\n",
            "Current benign train loss: 0.17952969670295715\n",
            "\n",
            "Total benign train accuarcy: 0.929\n",
            "Total benign train loss: 81.1268468350172\n",
            "\n",
            "[ Test epoch: 106 ]\n",
            "\n",
            "Test accuarcy: 85.78\n",
            "Test average loss: 0.004407037688791752\n",
            "\n",
            "[ Train epoch: 107 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.23121875524520874\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.890625\n",
            "Current benign train loss: 0.2348407804965973\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9296875\n",
            "Current benign train loss: 0.20310452580451965\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.16764038801193237\n",
            "\n",
            "Total benign train accuarcy: 0.929\n",
            "Total benign train loss: 80.63843262195587\n",
            "\n",
            "[ Test epoch: 107 ]\n",
            "\n",
            "Test accuarcy: 86.01\n",
            "Test average loss: 0.0045032379776239395\n",
            "\n",
            "[ Train epoch: 108 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.1921280026435852\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.09980442374944687\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.23889893293380737\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8984375\n",
            "Current benign train loss: 0.29078909754753113\n",
            "\n",
            "Total benign train accuarcy: 0.934\n",
            "Total benign train loss: 75.50184593349695\n",
            "\n",
            "[ Test epoch: 108 ]\n",
            "\n",
            "Test accuarcy: 86.11\n",
            "Test average loss: 0.004537521559000016\n",
            "\n",
            "[ Train epoch: 109 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9140625\n",
            "Current benign train loss: 0.17784583568572998\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.3074251711368561\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.21761353313922882\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.90625\n",
            "Current benign train loss: 0.309527724981308\n",
            "\n",
            "Total benign train accuarcy: 0.938\n",
            "Total benign train loss: 72.98650684952736\n",
            "\n",
            "[ Test epoch: 109 ]\n",
            "\n",
            "Test accuarcy: 86.38\n",
            "Test average loss: 0.004520823767781258\n",
            "\n",
            "[ Train epoch: 110 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.18694257736206055\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.13600575923919678\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.23416244983673096\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.19992126524448395\n",
            "\n",
            "Total benign train accuarcy: 0.936\n",
            "Total benign train loss: 71.73452541977167\n",
            "\n",
            "[ Test epoch: 110 ]\n",
            "\n",
            "Test accuarcy: 85.69\n",
            "Test average loss: 0.00471419637799263\n",
            "\n",
            "[ Train epoch: 111 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.1266319751739502\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.16159246861934662\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.13110700249671936\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8984375\n",
            "Current benign train loss: 0.27624398469924927\n",
            "\n",
            "Total benign train accuarcy: 0.941\n",
            "Total benign train loss: 68.11120994389057\n",
            "\n",
            "[ Test epoch: 111 ]\n",
            "\n",
            "Test accuarcy: 85.96\n",
            "Test average loss: 0.004769044883549213\n",
            "\n",
            "[ Train epoch: 112 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.08370141685009003\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.11967173218727112\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.16162852942943573\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.1058410108089447\n",
            "\n",
            "Total benign train accuarcy: 0.941\n",
            "Total benign train loss: 66.62348185107112\n",
            "\n",
            "[ Test epoch: 112 ]\n",
            "\n",
            "Test accuarcy: 85.63\n",
            "Test average loss: 0.004806567369401455\n",
            "\n",
            "[ Train epoch: 113 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.15295405685901642\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.17181876301765442\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.22145266830921173\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.13978701829910278\n",
            "\n",
            "Total benign train accuarcy: 0.943\n",
            "Total benign train loss: 64.51392223313451\n",
            "\n",
            "[ Test epoch: 113 ]\n",
            "\n",
            "Test accuarcy: 85.98\n",
            "Test average loss: 0.004700400887429714\n",
            "\n",
            "[ Train epoch: 114 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.11663325130939484\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.14249807596206665\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.17755600810050964\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.14584185183048248\n",
            "\n",
            "Total benign train accuarcy: 0.946\n",
            "Total benign train loss: 62.55577213317156\n",
            "\n",
            "[ Test epoch: 114 ]\n",
            "\n",
            "Test accuarcy: 85.9\n",
            "Test average loss: 0.0048623982042074205\n",
            "\n",
            "[ Train epoch: 115 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.09603021293878555\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.10420133173465729\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.15829646587371826\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.18901561200618744\n",
            "\n",
            "Total benign train accuarcy: 0.945\n",
            "Total benign train loss: 61.5991369523108\n",
            "\n",
            "[ Test epoch: 115 ]\n",
            "\n",
            "Test accuarcy: 85.39\n",
            "Test average loss: 0.005128111217916012\n",
            "\n",
            "[ Train epoch: 116 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.15566658973693848\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.11311029642820358\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.1262940615415573\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.1054253801703453\n",
            "\n",
            "Total benign train accuarcy: 0.947\n",
            "Total benign train loss: 60.28752603754401\n",
            "\n",
            "[ Test epoch: 116 ]\n",
            "\n",
            "Test accuarcy: 85.28\n",
            "Test average loss: 0.005112067872285843\n",
            "\n",
            "[ Train epoch: 117 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.16196325421333313\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.182438462972641\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.12567313015460968\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.16043667495250702\n",
            "\n",
            "Total benign train accuarcy: 0.948\n",
            "Total benign train loss: 59.213994812220335\n",
            "\n",
            "[ Test epoch: 117 ]\n",
            "\n",
            "Test accuarcy: 85.87\n",
            "Test average loss: 0.004958455047011375\n",
            "\n",
            "[ Train epoch: 118 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.16508004069328308\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.1424393653869629\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.10661803185939789\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.14262746274471283\n",
            "\n",
            "Total benign train accuarcy: 0.948\n",
            "Total benign train loss: 58.17760928720236\n",
            "\n",
            "[ Test epoch: 118 ]\n",
            "\n",
            "Test accuarcy: 84.92\n",
            "Test average loss: 0.0051511206775903704\n",
            "\n",
            "[ Train epoch: 119 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.12232332676649094\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.11388427019119263\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.175237774848938\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9140625\n",
            "Current benign train loss: 0.2560648024082184\n",
            "\n",
            "Total benign train accuarcy: 0.95\n",
            "Total benign train loss: 57.32260260730982\n",
            "\n",
            "[ Test epoch: 119 ]\n",
            "\n",
            "Test accuarcy: 85.07\n",
            "Test average loss: 0.005443802991509437\n",
            "\n",
            "[ Train epoch: 120 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.15054553747177124\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.14501261711120605\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.14655256271362305\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.1834840029478073\n",
            "\n",
            "Total benign train accuarcy: 0.951\n",
            "Total benign train loss: 54.7886869572103\n",
            "\n",
            "[ Test epoch: 120 ]\n",
            "\n",
            "Test accuarcy: 85.49\n",
            "Test average loss: 0.005100676657259464\n",
            "\n",
            "[ Train epoch: 121 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.13889217376708984\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.12415680289268494\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.12532690167427063\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.14777597784996033\n",
            "\n",
            "Total benign train accuarcy: 0.954\n",
            "Total benign train loss: 52.270292431116104\n",
            "\n",
            "[ Test epoch: 121 ]\n",
            "\n",
            "Test accuarcy: 85.67\n",
            "Test average loss: 0.0050570633798837665\n",
            "\n",
            "[ Train epoch: 122 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.1342538446187973\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.16489796340465546\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.11934464424848557\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9296875\n",
            "Current benign train loss: 0.19824925065040588\n",
            "\n",
            "Total benign train accuarcy: 0.952\n",
            "Total benign train loss: 54.69014014303684\n",
            "\n",
            "[ Test epoch: 122 ]\n",
            "\n",
            "Test accuarcy: 85.69\n",
            "Test average loss: 0.005125106026232243\n",
            "\n",
            "[ Train epoch: 123 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.11966139078140259\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.11196789145469666\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.11993031948804855\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9296875\n",
            "Current benign train loss: 0.16172322630882263\n",
            "\n",
            "Total benign train accuarcy: 0.955\n",
            "Total benign train loss: 51.62283254414797\n",
            "\n",
            "[ Test epoch: 123 ]\n",
            "\n",
            "Test accuarcy: 85.98\n",
            "Test average loss: 0.005084412281215191\n",
            "\n",
            "[ Train epoch: 124 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.16234983503818512\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.2136611044406891\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.16845490038394928\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.10362491756677628\n",
            "\n",
            "Total benign train accuarcy: 0.954\n",
            "Total benign train loss: 51.17589729465544\n",
            "\n",
            "[ Test epoch: 124 ]\n",
            "\n",
            "Test accuarcy: 86.01\n",
            "Test average loss: 0.005034209527075291\n",
            "\n",
            "[ Train epoch: 125 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.0853932648897171\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.15696629881858826\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.14341948926448822\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.11156263947486877\n",
            "\n",
            "Total benign train accuarcy: 0.956\n",
            "Total benign train loss: 50.22059882059693\n",
            "\n",
            "[ Test epoch: 125 ]\n",
            "\n",
            "Test accuarcy: 86.08\n",
            "Test average loss: 0.005135359999537468\n",
            "\n",
            "[ Train epoch: 126 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.11761129647493362\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.08727261424064636\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.10468851774930954\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.16402925550937653\n",
            "\n",
            "Total benign train accuarcy: 0.955\n",
            "Total benign train loss: 51.14474938251078\n",
            "\n",
            "[ Test epoch: 126 ]\n",
            "\n",
            "Test accuarcy: 85.19\n",
            "Test average loss: 0.005433821764588356\n",
            "\n",
            "[ Train epoch: 127 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.12891466915607452\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.12334982305765152\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.09401702135801315\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.08959531038999557\n",
            "\n",
            "Total benign train accuarcy: 0.958\n",
            "Total benign train loss: 48.12188583239913\n",
            "\n",
            "[ Test epoch: 127 ]\n",
            "\n",
            "Test accuarcy: 85.31\n",
            "Test average loss: 0.005427766761183739\n",
            "\n",
            "[ Train epoch: 128 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.14636383950710297\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9140625\n",
            "Current benign train loss: 0.20387254655361176\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.1331735998392105\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9296875\n",
            "Current benign train loss: 0.1700841188430786\n",
            "\n",
            "Total benign train accuarcy: 0.957\n",
            "Total benign train loss: 48.60736711323261\n",
            "\n",
            "[ Test epoch: 128 ]\n",
            "\n",
            "Test accuarcy: 85.36\n",
            "Test average loss: 0.005493029661476612\n",
            "\n",
            "[ Train epoch: 129 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9296875\n",
            "Current benign train loss: 0.171173095703125\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.12176468968391418\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.1073949858546257\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.05190348997712135\n",
            "\n",
            "Total benign train accuarcy: 0.958\n",
            "Total benign train loss: 47.520829413086176\n",
            "\n",
            "[ Test epoch: 129 ]\n",
            "\n",
            "Test accuarcy: 84.85\n",
            "Test average loss: 0.005631060823798179\n",
            "\n",
            "[ Train epoch: 130 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.08468128740787506\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.09837936609983444\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.14177632331848145\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.15120555460453033\n",
            "\n",
            "Total benign train accuarcy: 0.958\n",
            "Total benign train loss: 48.38986443914473\n",
            "\n",
            "[ Test epoch: 130 ]\n",
            "\n",
            "Test accuarcy: 85.88\n",
            "Test average loss: 0.005311422345042229\n",
            "\n",
            "[ Train epoch: 131 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.0945390909910202\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.10041283816099167\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.14965792000293732\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.14065183699131012\n",
            "\n",
            "Total benign train accuarcy: 0.959\n",
            "Total benign train loss: 46.76649083942175\n",
            "\n",
            "[ Test epoch: 131 ]\n",
            "\n",
            "Test accuarcy: 86.11\n",
            "Test average loss: 0.005181084199249744\n",
            "\n",
            "[ Train epoch: 132 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.18421582877635956\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.13024671375751495\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.15953904390335083\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.15323328971862793\n",
            "\n",
            "Total benign train accuarcy: 0.96\n",
            "Total benign train loss: 46.08919198811054\n",
            "\n",
            "[ Test epoch: 132 ]\n",
            "\n",
            "Test accuarcy: 84.65\n",
            "Test average loss: 0.005913699898123741\n",
            "\n",
            "[ Train epoch: 133 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.11575602740049362\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.04784513637423515\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.07967779040336609\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.10213643312454224\n",
            "\n",
            "Total benign train accuarcy: 0.959\n",
            "Total benign train loss: 46.89792198128998\n",
            "\n",
            "[ Test epoch: 133 ]\n",
            "\n",
            "Test accuarcy: 85.44\n",
            "Test average loss: 0.005504284858703613\n",
            "\n",
            "[ Train epoch: 134 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.15201182663440704\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.07038287073373795\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.14057514071464539\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9296875\n",
            "Current benign train loss: 0.15403853356838226\n",
            "\n",
            "Total benign train accuarcy: 0.959\n",
            "Total benign train loss: 46.797624956816435\n",
            "\n",
            "[ Test epoch: 134 ]\n",
            "\n",
            "Test accuarcy: 85.23\n",
            "Test average loss: 0.005639335311949253\n",
            "\n",
            "[ Train epoch: 135 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.0749790370464325\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.06225673854351044\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.07690414786338806\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.11896724998950958\n",
            "\n",
            "Total benign train accuarcy: 0.962\n",
            "Total benign train loss: 44.079026360064745\n",
            "\n",
            "[ Test epoch: 135 ]\n",
            "\n",
            "Test accuarcy: 84.98\n",
            "Test average loss: 0.005663968203961849\n",
            "\n",
            "[ Train epoch: 136 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.10299048572778702\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.12294566631317139\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.0824674442410469\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.08893292397260666\n",
            "\n",
            "Total benign train accuarcy: 0.96\n",
            "Total benign train loss: 45.85289606265724\n",
            "\n",
            "[ Test epoch: 136 ]\n",
            "\n",
            "Test accuarcy: 84.32\n",
            "Test average loss: 0.006039510467648506\n",
            "\n",
            "[ Train epoch: 137 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.07791924476623535\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.1308019459247589\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.14204041659832\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.14710864424705505\n",
            "\n",
            "Total benign train accuarcy: 0.961\n",
            "Total benign train loss: 43.917629923671484\n",
            "\n",
            "[ Test epoch: 137 ]\n",
            "\n",
            "Test accuarcy: 84.39\n",
            "Test average loss: 0.005838183803856373\n",
            "\n",
            "[ Train epoch: 138 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.17755092680454254\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.0520142987370491\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.0687205120921135\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.08197736740112305\n",
            "\n",
            "Total benign train accuarcy: 0.96\n",
            "Total benign train loss: 44.9197554346174\n",
            "\n",
            "[ Test epoch: 138 ]\n",
            "\n",
            "Test accuarcy: 84.04\n",
            "Test average loss: 0.005920595987141133\n",
            "\n",
            "[ Train epoch: 139 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.11929404735565186\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.11701265722513199\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.09601081907749176\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.05003681033849716\n",
            "\n",
            "Total benign train accuarcy: 0.96\n",
            "Total benign train loss: 45.72260179743171\n",
            "\n",
            "[ Test epoch: 139 ]\n",
            "\n",
            "Test accuarcy: 85.29\n",
            "Test average loss: 0.00560848807990551\n",
            "\n",
            "[ Train epoch: 140 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.1438480019569397\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.06385549902915955\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.07847979664802551\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.15075518190860748\n",
            "\n",
            "Total benign train accuarcy: 0.961\n",
            "Total benign train loss: 43.802739482373\n",
            "\n",
            "[ Test epoch: 140 ]\n",
            "\n",
            "Test accuarcy: 85.2\n",
            "Test average loss: 0.005562423732876778\n",
            "\n",
            "[ Train epoch: 141 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.1289454996585846\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.10492628067731857\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.0512448251247406\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.12449958920478821\n",
            "\n",
            "Total benign train accuarcy: 0.962\n",
            "Total benign train loss: 43.19791014492512\n",
            "\n",
            "[ Test epoch: 141 ]\n",
            "\n",
            "Test accuarcy: 84.66\n",
            "Test average loss: 0.005850785136222839\n",
            "\n",
            "[ Train epoch: 142 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.16239766776561737\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.10005491971969604\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.12319138646125793\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.0929565578699112\n",
            "\n",
            "Total benign train accuarcy: 0.963\n",
            "Total benign train loss: 42.12756613083184\n",
            "\n",
            "[ Test epoch: 142 ]\n",
            "\n",
            "Test accuarcy: 84.23\n",
            "Test average loss: 0.006015162426233292\n",
            "\n",
            "[ Train epoch: 143 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.13322658836841583\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9296875\n",
            "Current benign train loss: 0.10804206132888794\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.1554688662290573\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.06506680697202682\n",
            "\n",
            "Total benign train accuarcy: 0.961\n",
            "Total benign train loss: 44.02945267036557\n",
            "\n",
            "[ Test epoch: 143 ]\n",
            "\n",
            "Test accuarcy: 84.74\n",
            "Test average loss: 0.006015725764632225\n",
            "\n",
            "[ Train epoch: 144 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.12099234014749527\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.12559552490711212\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.1268012970685959\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.09641194343566895\n",
            "\n",
            "Total benign train accuarcy: 0.962\n",
            "Total benign train loss: 44.16101343743503\n",
            "\n",
            "[ Test epoch: 144 ]\n",
            "\n",
            "Test accuarcy: 84.5\n",
            "Test average loss: 0.005884303991496563\n",
            "\n",
            "[ Train epoch: 145 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.14774759113788605\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.042330823838710785\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.12225068360567093\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02696056477725506\n",
            "\n",
            "Total benign train accuarcy: 0.961\n",
            "Total benign train loss: 43.312079042196274\n",
            "\n",
            "[ Test epoch: 145 ]\n",
            "\n",
            "Test accuarcy: 83.81\n",
            "Test average loss: 0.006130756491422653\n",
            "\n",
            "[ Train epoch: 146 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.12007623165845871\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.10694436728954315\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.07516986131668091\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.07270071655511856\n",
            "\n",
            "Total benign train accuarcy: 0.964\n",
            "Total benign train loss: 40.680732833221555\n",
            "\n",
            "[ Test epoch: 146 ]\n",
            "\n",
            "Test accuarcy: 84.97\n",
            "Test average loss: 0.005766387918591499\n",
            "\n",
            "[ Train epoch: 147 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.09950461983680725\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.08293302357196808\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.07586593925952911\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.11466904729604721\n",
            "\n",
            "Total benign train accuarcy: 0.961\n",
            "Total benign train loss: 43.77335531078279\n",
            "\n",
            "[ Test epoch: 147 ]\n",
            "\n",
            "Test accuarcy: 84.96\n",
            "Test average loss: 0.0056650379821658135\n",
            "\n",
            "[ Train epoch: 148 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.10579454898834229\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9453125\n",
            "Current benign train loss: 0.13985887169837952\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.1092170923948288\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.14059637486934662\n",
            "\n",
            "Total benign train accuarcy: 0.962\n",
            "Total benign train loss: 42.048354268074036\n",
            "\n",
            "[ Test epoch: 148 ]\n",
            "\n",
            "Test accuarcy: 84.43\n",
            "Test average loss: 0.006260382294654846\n",
            "\n",
            "[ Train epoch: 149 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.06486987322568893\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.07525090873241425\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.13137462735176086\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.07510647177696228\n",
            "\n",
            "Total benign train accuarcy: 0.963\n",
            "Total benign train loss: 43.27350692451\n",
            "\n",
            "[ Test epoch: 149 ]\n",
            "\n",
            "Test accuarcy: 84.51\n",
            "Test average loss: 0.0061114528775215145\n",
            "\n",
            "[ Train epoch: 150 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.051166657358407974\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.953125\n",
            "Current benign train loss: 0.13323600590229034\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.048056479543447495\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.03378288447856903\n",
            "\n",
            "Total benign train accuarcy: 0.973\n",
            "Total benign train loss: 31.104142766445875\n",
            "\n",
            "[ Test epoch: 150 ]\n",
            "\n",
            "Test accuarcy: 86.04\n",
            "Test average loss: 0.005225010775029659\n",
            "\n",
            "[ Train epoch: 151 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.06462309509515762\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.04857957735657692\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.12816022336483002\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.04595467448234558\n",
            "\n",
            "Total benign train accuarcy: 0.98\n",
            "Total benign train loss: 23.8820456052199\n",
            "\n",
            "[ Test epoch: 151 ]\n",
            "\n",
            "Test accuarcy: 86.31\n",
            "Test average loss: 0.005213292191922665\n",
            "\n",
            "[ Train epoch: 152 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.050475649535655975\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.10133225470781326\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.06268423050642014\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.04688321799039841\n",
            "\n",
            "Total benign train accuarcy: 0.982\n",
            "Total benign train loss: 22.739901553839445\n",
            "\n",
            "[ Test epoch: 152 ]\n",
            "\n",
            "Test accuarcy: 86.13\n",
            "Test average loss: 0.005253481069207191\n",
            "\n",
            "[ Train epoch: 153 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.016595596447587013\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.039058785885572433\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.03568946197628975\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03362908959388733\n",
            "\n",
            "Total benign train accuarcy: 0.982\n",
            "Total benign train loss: 21.10348037444055\n",
            "\n",
            "[ Test epoch: 153 ]\n",
            "\n",
            "Test accuarcy: 86.39\n",
            "Test average loss: 0.0053014162555336955\n",
            "\n",
            "[ Train epoch: 154 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.06797309964895248\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.04189392924308777\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.02772580087184906\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.045090027153491974\n",
            "\n",
            "Total benign train accuarcy: 0.983\n",
            "Total benign train loss: 20.406924131326377\n",
            "\n",
            "[ Test epoch: 154 ]\n",
            "\n",
            "Test accuarcy: 86.63\n",
            "Test average loss: 0.005320139841735363\n",
            "\n",
            "[ Train epoch: 155 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.047871947288513184\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03997527435421944\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.04212566465139389\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.058028195053339005\n",
            "\n",
            "Total benign train accuarcy: 0.984\n",
            "Total benign train loss: 18.796293848194182\n",
            "\n",
            "[ Test epoch: 155 ]\n",
            "\n",
            "Test accuarcy: 86.55\n",
            "Test average loss: 0.005325264367461204\n",
            "\n",
            "[ Train epoch: 156 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.017663147300481796\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.05141916126012802\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.007183403242379427\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.06402262300252914\n",
            "\n",
            "Total benign train accuarcy: 0.985\n",
            "Total benign train loss: 18.03014566982165\n",
            "\n",
            "[ Test epoch: 156 ]\n",
            "\n",
            "Test accuarcy: 86.51\n",
            "Test average loss: 0.005421166910231113\n",
            "\n",
            "[ Train epoch: 157 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.10018966346979141\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.0871262177824974\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.0739576518535614\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.027709443122148514\n",
            "\n",
            "Total benign train accuarcy: 0.985\n",
            "Total benign train loss: 17.778326028957963\n",
            "\n",
            "[ Test epoch: 157 ]\n",
            "\n",
            "Test accuarcy: 86.49\n",
            "Test average loss: 0.00542535200715065\n",
            "\n",
            "[ Train epoch: 158 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.01746593974530697\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.0343906544148922\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.04371827468276024\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.04393703490495682\n",
            "\n",
            "Total benign train accuarcy: 0.986\n",
            "Total benign train loss: 17.162657842040062\n",
            "\n",
            "[ Test epoch: 158 ]\n",
            "\n",
            "Test accuarcy: 86.64\n",
            "Test average loss: 0.005406360666453838\n",
            "\n",
            "[ Train epoch: 159 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02639843337237835\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.016280589625239372\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.022920403629541397\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.04119424149394035\n",
            "\n",
            "Total benign train accuarcy: 0.986\n",
            "Total benign train loss: 16.94670376041904\n",
            "\n",
            "[ Test epoch: 159 ]\n",
            "\n",
            "Test accuarcy: 86.49\n",
            "Test average loss: 0.005426593424379826\n",
            "\n",
            "[ Train epoch: 160 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.008440672419965267\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.06868704408407211\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02838287502527237\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.020201239734888077\n",
            "\n",
            "Total benign train accuarcy: 0.987\n",
            "Total benign train loss: 15.383830700535327\n",
            "\n",
            "[ Test epoch: 160 ]\n",
            "\n",
            "Test accuarcy: 86.55\n",
            "Test average loss: 0.005526955287158489\n",
            "\n",
            "[ Train epoch: 161 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03963930904865265\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.05417020618915558\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.07960471510887146\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.03745991364121437\n",
            "\n",
            "Total benign train accuarcy: 0.987\n",
            "Total benign train loss: 15.665332990232855\n",
            "\n",
            "[ Test epoch: 161 ]\n",
            "\n",
            "Test accuarcy: 86.72\n",
            "Test average loss: 0.005513266038894653\n",
            "\n",
            "[ Train epoch: 162 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.037396449595689774\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.037446603178977966\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.03415854647755623\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.04543992131948471\n",
            "\n",
            "Total benign train accuarcy: 0.987\n",
            "Total benign train loss: 15.956551288254559\n",
            "\n",
            "[ Test epoch: 162 ]\n",
            "\n",
            "Test accuarcy: 86.63\n",
            "Test average loss: 0.005520652340352535\n",
            "\n",
            "[ Train epoch: 163 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02946259267628193\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.022122742608189583\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.01380946021527052\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.038740046322345734\n",
            "\n",
            "Total benign train accuarcy: 0.988\n",
            "Total benign train loss: 14.893999822437763\n",
            "\n",
            "[ Test epoch: 163 ]\n",
            "\n",
            "Test accuarcy: 86.68\n",
            "Test average loss: 0.0055527612388134\n",
            "\n",
            "[ Train epoch: 164 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.01262427307665348\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03959263861179352\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.05081132799386978\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.028398729860782623\n",
            "\n",
            "Total benign train accuarcy: 0.988\n",
            "Total benign train loss: 14.71830884180963\n",
            "\n",
            "[ Test epoch: 164 ]\n",
            "\n",
            "Test accuarcy: 86.63\n",
            "Test average loss: 0.005560235433280468\n",
            "\n",
            "[ Train epoch: 165 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.021521175280213356\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.02401117980480194\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.052225057035684586\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.009638429619371891\n",
            "\n",
            "Total benign train accuarcy: 0.989\n",
            "Total benign train loss: 14.133811136242002\n",
            "\n",
            "[ Test epoch: 165 ]\n",
            "\n",
            "Test accuarcy: 86.71\n",
            "Test average loss: 0.005558352918922901\n",
            "\n",
            "[ Train epoch: 166 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.011064511723816395\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02325759455561638\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03180994838476181\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.014159997925162315\n",
            "\n",
            "Total benign train accuarcy: 0.989\n",
            "Total benign train loss: 13.692467758897692\n",
            "\n",
            "[ Test epoch: 166 ]\n",
            "\n",
            "Test accuarcy: 86.55\n",
            "Test average loss: 0.005615700761973858\n",
            "\n",
            "[ Train epoch: 167 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.01790875755250454\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.01186244934797287\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03754135221242905\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.024195125326514244\n",
            "\n",
            "Total benign train accuarcy: 0.989\n",
            "Total benign train loss: 13.614016393199563\n",
            "\n",
            "[ Test epoch: 167 ]\n",
            "\n",
            "Test accuarcy: 86.67\n",
            "Test average loss: 0.005593889503180981\n",
            "\n",
            "[ Train epoch: 168 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.035463299602270126\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.11070273071527481\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.028720110654830933\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.06458592414855957\n",
            "\n",
            "Total benign train accuarcy: 0.989\n",
            "Total benign train loss: 14.237451414577663\n",
            "\n",
            "[ Test epoch: 168 ]\n",
            "\n",
            "Test accuarcy: 86.68\n",
            "Test average loss: 0.005669423598051071\n",
            "\n",
            "[ Train epoch: 169 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.054897237569093704\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.021116137504577637\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.019641544669866562\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0204535610973835\n",
            "\n",
            "Total benign train accuarcy: 0.99\n",
            "Total benign train loss: 12.39237707061693\n",
            "\n",
            "[ Test epoch: 169 ]\n",
            "\n",
            "Test accuarcy: 86.66\n",
            "Test average loss: 0.005759960344433784\n",
            "\n",
            "[ Train epoch: 170 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.025904396548867226\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.018547585234045982\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02139200083911419\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.04572082310914993\n",
            "\n",
            "Total benign train accuarcy: 0.989\n",
            "Total benign train loss: 13.065042952541262\n",
            "\n",
            "[ Test epoch: 170 ]\n",
            "\n",
            "Test accuarcy: 86.62\n",
            "Test average loss: 0.005760418419539929\n",
            "\n",
            "[ Train epoch: 171 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.06547927111387253\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.07498762756586075\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03364096209406853\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.009817778132855892\n",
            "\n",
            "Total benign train accuarcy: 0.989\n",
            "Total benign train loss: 13.087619576370344\n",
            "\n",
            "[ Test epoch: 171 ]\n",
            "\n",
            "Test accuarcy: 86.41\n",
            "Test average loss: 0.005840444257855415\n",
            "\n",
            "[ Train epoch: 172 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.026924550533294678\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02016959898173809\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.024336600676178932\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.04587272182106972\n",
            "\n",
            "Total benign train accuarcy: 0.99\n",
            "Total benign train loss: 11.885850452352315\n",
            "\n",
            "[ Test epoch: 172 ]\n",
            "\n",
            "Test accuarcy: 86.51\n",
            "Test average loss: 0.0057741980105638505\n",
            "\n",
            "[ Train epoch: 173 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.010909919627010822\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.020529402419924736\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.06024135649204254\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0163599643856287\n",
            "\n",
            "Total benign train accuarcy: 0.99\n",
            "Total benign train loss: 12.457603122107685\n",
            "\n",
            "[ Test epoch: 173 ]\n",
            "\n",
            "Test accuarcy: 86.43\n",
            "Test average loss: 0.005763834558427334\n",
            "\n",
            "[ Train epoch: 174 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.05477999150753021\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.007651768624782562\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.027701769024133682\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0091220922768116\n",
            "\n",
            "Total benign train accuarcy: 0.99\n",
            "Total benign train loss: 11.794643982779235\n",
            "\n",
            "[ Test epoch: 174 ]\n",
            "\n",
            "Test accuarcy: 86.38\n",
            "Test average loss: 0.005862478747963905\n",
            "\n",
            "[ Train epoch: 175 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.022139551118016243\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.01850704476237297\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03205135837197304\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.022660506889224052\n",
            "\n",
            "Total benign train accuarcy: 0.99\n",
            "Total benign train loss: 11.94245972763747\n",
            "\n",
            "[ Test epoch: 175 ]\n",
            "\n",
            "Test accuarcy: 86.63\n",
            "Test average loss: 0.005908784762024879\n",
            "\n",
            "[ Train epoch: 176 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.015412986278533936\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.03344496712088585\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.026894085109233856\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.06044081598520279\n",
            "\n",
            "Total benign train accuarcy: 0.991\n",
            "Total benign train loss: 11.500249189324677\n",
            "\n",
            "[ Test epoch: 176 ]\n",
            "\n",
            "Test accuarcy: 86.69\n",
            "Test average loss: 0.005818697720766068\n",
            "\n",
            "[ Train epoch: 177 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.007694764994084835\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.014652932994067669\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.04547103866934776\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.024051208049058914\n",
            "\n",
            "Total benign train accuarcy: 0.99\n",
            "Total benign train loss: 11.637698080623522\n",
            "\n",
            "[ Test epoch: 177 ]\n",
            "\n",
            "Test accuarcy: 86.65\n",
            "Test average loss: 0.005870151562988758\n",
            "\n",
            "[ Train epoch: 178 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.01405230350792408\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.011769574135541916\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.011680916883051395\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.019990982487797737\n",
            "\n",
            "Total benign train accuarcy: 0.991\n",
            "Total benign train loss: 11.89491306245327\n",
            "\n",
            "[ Test epoch: 178 ]\n",
            "\n",
            "Test accuarcy: 86.52\n",
            "Test average loss: 0.005890809963643551\n",
            "\n",
            "[ Train epoch: 179 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.011504970490932465\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.011838180013000965\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.009115638211369514\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02685510739684105\n",
            "\n",
            "Total benign train accuarcy: 0.991\n",
            "Total benign train loss: 11.212393315974623\n",
            "\n",
            "[ Test epoch: 179 ]\n",
            "\n",
            "Test accuarcy: 86.61\n",
            "Test average loss: 0.005909827008843422\n",
            "\n",
            "[ Train epoch: 180 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.026987597346305847\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.024527234956622124\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.010701592080295086\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.03876562789082527\n",
            "\n",
            "Total benign train accuarcy: 0.991\n",
            "Total benign train loss: 10.920547930756584\n",
            "\n",
            "[ Test epoch: 180 ]\n",
            "\n",
            "Test accuarcy: 86.61\n",
            "Test average loss: 0.005899122045934201\n",
            "\n",
            "[ Train epoch: 181 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.04591319337487221\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.017501873895525932\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.04511726275086403\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03370189294219017\n",
            "\n",
            "Total benign train accuarcy: 0.991\n",
            "Total benign train loss: 11.0783929429017\n",
            "\n",
            "[ Test epoch: 181 ]\n",
            "\n",
            "Test accuarcy: 86.51\n",
            "Test average loss: 0.005941684854030609\n",
            "\n",
            "[ Train epoch: 182 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.056440237909555435\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03228091821074486\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.015081089921295643\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.047412898391485214\n",
            "\n",
            "Total benign train accuarcy: 0.991\n",
            "Total benign train loss: 10.683679239824414\n",
            "\n",
            "[ Test epoch: 182 ]\n",
            "\n",
            "Test accuarcy: 86.61\n",
            "Test average loss: 0.0059414059534668925\n",
            "\n",
            "[ Train epoch: 183 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.031814392656087875\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.039253655821084976\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.03334105387330055\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.03021581470966339\n",
            "\n",
            "Total benign train accuarcy: 0.992\n",
            "Total benign train loss: 10.397833655355498\n",
            "\n",
            "[ Test epoch: 183 ]\n",
            "\n",
            "Test accuarcy: 86.38\n",
            "Test average loss: 0.005959235793352127\n",
            "\n",
            "[ Train epoch: 184 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0167628675699234\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.030468309298157692\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.014066219329833984\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.04152536392211914\n",
            "\n",
            "Total benign train accuarcy: 0.992\n",
            "Total benign train loss: 10.005341791780666\n",
            "\n",
            "[ Test epoch: 184 ]\n",
            "\n",
            "Test accuarcy: 86.53\n",
            "Test average loss: 0.00598529191762209\n",
            "\n",
            "[ Train epoch: 185 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03482278808951378\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.007208955939859152\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.011374106630682945\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.005325267091393471\n",
            "\n",
            "Total benign train accuarcy: 0.992\n",
            "Total benign train loss: 10.234249313129112\n",
            "\n",
            "[ Test epoch: 185 ]\n",
            "\n",
            "Test accuarcy: 86.71\n",
            "Test average loss: 0.005967722524702549\n",
            "\n",
            "[ Train epoch: 186 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.019696805626153946\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02569417469203472\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.009919621981680393\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03040660172700882\n",
            "\n",
            "Total benign train accuarcy: 0.992\n",
            "Total benign train loss: 10.367089056409895\n",
            "\n",
            "[ Test epoch: 186 ]\n",
            "\n",
            "Test accuarcy: 86.51\n",
            "Test average loss: 0.005948521019518376\n",
            "\n",
            "[ Train epoch: 187 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.014251536689698696\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.045878492295742035\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.01551857776939869\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.03908431529998779\n",
            "\n",
            "Total benign train accuarcy: 0.991\n",
            "Total benign train loss: 10.331622889265418\n",
            "\n",
            "[ Test epoch: 187 ]\n",
            "\n",
            "Test accuarcy: 86.62\n",
            "Test average loss: 0.005983330838382244\n",
            "\n",
            "[ Train epoch: 188 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.09058272838592529\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.029870426282286644\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.052950311452150345\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.006423624698072672\n",
            "\n",
            "Total benign train accuarcy: 0.991\n",
            "Total benign train loss: 10.728275439236313\n",
            "\n",
            "[ Test epoch: 188 ]\n",
            "\n",
            "Test accuarcy: 86.78\n",
            "Test average loss: 0.006023211865127087\n",
            "\n",
            "[ Train epoch: 189 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.006528170313686132\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0040199835784733295\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.06379672139883041\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.01941339485347271\n",
            "\n",
            "Total benign train accuarcy: 0.992\n",
            "Total benign train loss: 9.949252265738323\n",
            "\n",
            "[ Test epoch: 189 ]\n",
            "\n",
            "Test accuarcy: 86.7\n",
            "Test average loss: 0.006061672107875347\n",
            "\n",
            "[ Train epoch: 190 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.06345241516828537\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.004432866349816322\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.07816547155380249\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.012774849310517311\n",
            "\n",
            "Total benign train accuarcy: 0.992\n",
            "Total benign train loss: 10.143659100402147\n",
            "\n",
            "[ Test epoch: 190 ]\n",
            "\n",
            "Test accuarcy: 86.61\n",
            "Test average loss: 0.006050442998111248\n",
            "\n",
            "[ Train epoch: 191 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.009796205908060074\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.034871723502874374\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02655511535704136\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.01977631263434887\n",
            "\n",
            "Total benign train accuarcy: 0.992\n",
            "Total benign train loss: 10.315494072623551\n",
            "\n",
            "[ Test epoch: 191 ]\n",
            "\n",
            "Test accuarcy: 86.6\n",
            "Test average loss: 0.006071469070017338\n",
            "\n",
            "[ Train epoch: 192 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.032787468284368515\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.016273178160190582\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.007121828850358725\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.017401529476046562\n",
            "\n",
            "Total benign train accuarcy: 0.992\n",
            "Total benign train loss: 10.11965218023397\n",
            "\n",
            "[ Test epoch: 192 ]\n",
            "\n",
            "Test accuarcy: 86.38\n",
            "Test average loss: 0.006093051214516163\n",
            "\n",
            "[ Train epoch: 193 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.017800631001591682\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.004591754171997309\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9765625\n",
            "Current benign train loss: 0.07078683376312256\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.029047511518001556\n",
            "\n",
            "Total benign train accuarcy: 0.992\n",
            "Total benign train loss: 9.803980150958523\n",
            "\n",
            "[ Test epoch: 193 ]\n",
            "\n",
            "Test accuarcy: 86.57\n",
            "Test average loss: 0.006095147624611855\n",
            "\n",
            "[ Train epoch: 194 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.04249493032693863\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02286345697939396\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.031248416751623154\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.024298429489135742\n",
            "\n",
            "Total benign train accuarcy: 0.992\n",
            "Total benign train loss: 10.142486060969532\n",
            "\n",
            "[ Test epoch: 194 ]\n",
            "\n",
            "Test accuarcy: 86.56\n",
            "Test average loss: 0.006125251586735249\n",
            "\n",
            "[ Train epoch: 195 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.01118751335889101\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.012748494744300842\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.031242405995726585\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.00881175696849823\n",
            "\n",
            "Total benign train accuarcy: 0.992\n",
            "Total benign train loss: 9.459691402502358\n",
            "\n",
            "[ Test epoch: 195 ]\n",
            "\n",
            "Test accuarcy: 86.36\n",
            "Test average loss: 0.0061867118999361995\n",
            "\n",
            "[ Train epoch: 196 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.009058472700417042\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.0166146419942379\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02058292366564274\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.02396375685930252\n",
            "\n",
            "Total benign train accuarcy: 0.993\n",
            "Total benign train loss: 9.090726288966835\n",
            "\n",
            "[ Test epoch: 196 ]\n",
            "\n",
            "Test accuarcy: 86.38\n",
            "Test average loss: 0.006176758621633052\n",
            "\n",
            "[ Train epoch: 197 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.016600070521235466\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.024945223703980446\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.00914589874446392\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.0305167306214571\n",
            "\n",
            "Total benign train accuarcy: 0.992\n",
            "Total benign train loss: 9.35737690643873\n",
            "\n",
            "[ Test epoch: 197 ]\n",
            "\n",
            "Test accuarcy: 86.71\n",
            "Test average loss: 0.006082064865529537\n",
            "\n",
            "[ Train epoch: 198 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.011660112999379635\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.039356034249067307\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.020051181316375732\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.016111429780721664\n",
            "\n",
            "Total benign train accuarcy: 0.993\n",
            "Total benign train loss: 9.079431105172262\n",
            "\n",
            "[ Test epoch: 198 ]\n",
            "\n",
            "Test accuarcy: 86.57\n",
            "Test average loss: 0.006148975396156311\n",
            "\n",
            "[ Train epoch: 199 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.984375\n",
            "Current benign train loss: 0.04798869788646698\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.06337221711874008\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.0037135013844817877\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.016644276678562164\n",
            "\n",
            "Total benign train accuarcy: 0.993\n",
            "Total benign train loss: 8.711560355266556\n",
            "\n",
            "[ Test epoch: 199 ]\n",
            "\n",
            "Test accuarcy: 86.36\n",
            "Test average loss: 0.006216101142764091\n",
            "\n",
            "[ Train epoch: 200 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.010630275122821331\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.030483802780508995\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.01596316322684288\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.9921875\n",
            "Current benign train loss: 0.020997408777475357\n",
            "\n",
            "Total benign train accuarcy: 0.993\n",
            "Total benign train loss: 8.202197877224535\n",
            "\n",
            "[ Test epoch: 200 ]\n",
            "\n",
            "Test accuarcy: 86.47\n",
            "Test average loss: 0.006198173986375332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "OlUhxEcpclOP",
        "outputId": "6c593be8-9bc9-4562-fe4d-c3218b7cdd9c"
      },
      "source": [
        "x = np.arange(200)\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
        "ax[0].plot(x, history['train_loss'], label='train loss')\n",
        "# ax[0].plot(x, history['test_loss'], label='test loss')\n",
        "ax[1].plot(x, history['test_acc'], label='test accuracy')\n",
        "\n",
        "for i in range (0,2):\n",
        "  lines, labels = ax[i].get_legend_handles_labels()\n",
        "  ax[i].legend(lines, labels, loc = 'upper right')\n",
        "  ax[i].set_xlabel('iteration', fontsize=16)\n",
        "\n",
        "ax[0].set_title('Total Loss in train', fontsize=20)\n",
        "ax[1].set_title('Accuracy in test', fontsize=20)\n",
        "ax[0].set_ylabel('loss', fontsize=16)\n",
        "ax[1].set_ylabel('accuracy', fontsize=16)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}